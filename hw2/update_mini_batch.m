function [new_weights, new_biases, return_velocity] = update_mini_batch(x_batch, y_batch, weights, biases, eta, nodeLayers, cost_function, momentum, velocity, lambda, num_examples)    new_weights = {};    new_biases = {};    # Get the change in the weights and biases for the given batch    [nabla_w, nabla_b] = backprop(x_batch, y_batch, weights, biases, nodeLayers, cost_function);    # Update the weights and biases on each layer    for layer_idx = 1:columns(weights)        lambda_factor = (1 - eta * lambda / num_examples);        return_velocity{layer_idx + 1} = momentum * velocity{layer_idx + 1} - eta/rows(x_batch) * nabla_w{layer_idx + 1}';        # new weight is wL - (learning rate / N) * dw/dc(L+!)        new_weights{layer_idx} = lambda_factor * weights{layer_idx} + return_velocity{layer_idx + 1};        # new bias is bL - (learning rate / N) * db/dc(L+!)        new_biases{layer_idx} = biases{layer_idx} - eta/rows(x_batch) * nabla_b{layer_idx + 1};    endforendfunction