function [nabla_w, nabla_b] = backprop (x_batch, y_batch, weights, biases, nodeLayers, cost_function)    # Initialize nabla b and nabla w to the size of the weight and bias layers, as 0's    nabla_b = get_nabla_b(nodeLayers);    nabla_w = get_nabla_w(nodeLayers);        # Get the zscore and activations for each later    [zs, activations] = feedforward(x_batch, weights, biases);    output_error = get_output_error(zs, activations, y_batch, cost_function);        # The b error is the sum over all b errors for this batch's output (as opposed to just the regular output error)    nabla_b(end) = sum(output_error, 2);        # W is the matrix product of the output error at layer l and the activation at layer l - 1    nabla_w(end) = output_error * activations{end - 1}';        # Traverse all layers from the end of the network back, computing the output error at each layer    for layer = 1:length(nodeLayers) - 2        z = zs{end - layer};        sp = sigmoid_prime(z);                # Output error is w(L + 1) * outputerror(L + 1) (circle dot) sigmoid prime(zL)        output_error = weights{end - layer + 1} * output_error .* sp;                # dc/db is just the output error        nabla_b{end - layer} = sum(output_error, 2);                # dc/dw is the output error(L) * a(L-1)        nabla_w{end - layer} = output_error * activations{end-layer-1}';    endforendfunction